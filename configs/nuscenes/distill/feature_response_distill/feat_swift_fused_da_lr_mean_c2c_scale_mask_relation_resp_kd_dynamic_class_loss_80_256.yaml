# Detection Space
voxel_size: [0.075, 0.075, 0.2]
point_cloud_range: [-54.0, -54.0, -5.0, 54.0, 54.0, 3.0]

# Data
data:
  samples_per_gpu: 4
  workers_per_gpu: 10
  train:
    dataset:
      ann_file: ${dataset_root + 'nuscenes_infos_train.pkl'}
  val:
    ann_file: ${dataset_root + 'nuscenes_infos_val.pkl'}
  test:
    ann_file: ${dataset_root + 'nuscenes_infos_val.pkl'}

# Image size
image_size: [256, 704]

# Augment
augment2d:
  resize: [[0.38, 0.55], [0.48, 0.48]]

augment3d:
  scale: [0.9, 1.1]
  rotate: [0, 0]
  translate: 0.5

# Model
model:
  type: BEVResponseDistillerFusedLRC2CMaskScaleRelation
  # Teacher
  # encoders
  encoders:
    camera:
      backbone:
        type: SwinTransformer
        embed_dims: 96
        depths: [2, 2, 6, 2]
        num_heads: [3, 6, 12, 24]
        window_size: 7
        mlp_ratio: 4
        qkv_bias: true
        qk_scale: null
        drop_rate: 0.
        attn_drop_rate: 0.
        drop_path_rate: 0.2
        patch_norm: true
        out_indices: [1, 2, 3]
        with_cp: false
        convert_weights: true
        init_cfg:
          type: Pretrained
          checkpoint: https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth
      neck:
        type: GeneralizedLSSFPN
        in_channels: [192, 384, 768]
        out_channels: 256
        start_level: 0
        num_outs: 3
        norm_cfg:
          type: BN2d
          requires_grad: true
        act_cfg:
          type: ReLU
          inplace: true
        upsample_cfg:
          mode: bilinear
          align_corners: false
      vtransform:
        type: DepthLSSTransform
        in_channels: 256
        out_channels: 80
        image_size: ${image_size}
        feature_size: ${[image_size[0] // 8, image_size[1] // 8]}
        xbound: [-54.0, 54.0, 0.3]
        ybound: [-54.0, 54.0, 0.3]
        zbound: [-10.0, 10.0, 20.0]
        dbound: [1.0, 60.0, 0.5]
        downsample: 2
    lidar:
      voxelize:
        max_num_points: 10
        point_cloud_range: ${point_cloud_range}
        voxel_size: ${voxel_size}
        max_voxels: [120000, 160000]
      backbone:
        type: SparseEncoder
        in_channels: 5
        sparse_shape: [1440, 1440, 41]
        output_channels: 128
        order:
          - conv
          - norm
          - act
        encoder_channels:
          - [16, 16, 32]
          - [32, 32, 64]
          - [64, 64, 128]
          - [128, 128]
        encoder_paddings:
          - [0, 0, 1]
          - [0, 0, 1]
          - [0, 0, [1, 1, 0]]
          - [0, 0]
        block_type: basicblock
  # fuser
  fuser:
    type: ConvFuser
    in_channels: [80, 256]
    out_channels: 256
  # decoder
  decoder:
    backbone:
      type: SECOND
      in_channels: 256
      out_channels: [ 128, 256 ]
      layer_nums: [ 5, 5 ]
      layer_strides: [ 1, 2 ]
      norm_cfg:
        type: BN
        eps: 1.0e-3
        momentum: 0.01
      conv_cfg:
        type: Conv2d
        bias: false
    neck:
      type: SECONDFPN
      in_channels: [ 128, 256 ]
      out_channels: [ 256, 256 ]
      upsample_strides: [ 1, 2 ]
      norm_cfg:
        type: BN
        eps: 1.0e-3
        momentum: 0.01
      upsample_cfg:
        type: deconv
        bias: false
      use_conv_for_no_stride: true
  # heads
  heads:
    object:
      type: TransFusionHead
      in_channels: 512
      num_proposals: 200
      auxiliary: true
      hidden_channel: 128
      num_classes: 10
      num_decoder_layers: 1
      num_heads: 8
      nms_kernel_size: 3
      ffn_channel: 256
      dropout: 0.1
      bn_momentum: 0.1
      activation: relu
      train_cfg:
        dataset: nuScenes
        point_cloud_range: ${point_cloud_range}
        grid_size: [ 1440, 1440, 41 ]
        voxel_size: ${voxel_size}
        out_size_factor: 8
        gaussian_overlap: 0.1
        min_radius: 2
        pos_weight: -1
        code_weights: [ 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2 ]
        assigner:
          type: HungarianAssigner3D
          iou_calculator:
            type: BboxOverlaps3D
            coordinate: lidar
          cls_cost:
            type: FocalLossCost
            gamma: 2.0
            alpha: 0.25
            weight: 0.15
          reg_cost:
            type: BBoxBEVL1Cost
            weight: 0.25
          iou_cost:
            type: IoU3DCost
            weight: 0.25
      test_cfg:
        dataset: nuScenes
        grid_size: [ 1440, 1440, 41 ]
        out_size_factor: 8
        voxel_size: ${voxel_size[:2]}
        pc_range: ${point_cloud_range[:2]}
        nms_type: null
      common_heads:
        center: [ 2, 2 ]
        height: [ 1, 2 ]
        dim: [ 3, 2 ]
        rot: [ 2, 2 ]
        vel: [ 2, 2 ]
      bbox_coder:
        type: TransFusionBBoxCoder
        pc_range: ${point_cloud_range[:2]}
        post_center_range: [ -61.2, -61.2, -10.0, 61.2, 61.2, 10.0 ]
        score_threshold: 0.0
        out_size_factor: 8
        voxel_size: ${voxel_size[:2]}
        code_size: 10
#      tasks:
#        - [ "car" ]
#        - [ "truck", "construction_vehicle" ]
#        - [ "bus", "trailer" ]
#        - [ "barrier" ]
#        - [ "motorcycle", "bicycle" ]
#        - [ "pedestrian", "traffic_cone" ]
      loss_cls:
        type: FocalLoss
        use_sigmoid: true
        gamma: 2.0
        alpha: 0.25
        reduction: mean
        loss_weight: 1.0
      loss_heatmap:
        type: GaussianFocalLoss
        reduction: mean
        loss_weight: 1.0
      loss_bbox:
        type: L1Loss
        reduction: mean
        loss_weight: 0.25
#      norm_bbox: true
  
  # Student
  encoders_student:
    lidar:
      voxelize:
        max_num_points: 10
        point_cloud_range: ${point_cloud_range}
        voxel_size: ${voxel_size}
        max_voxels: [ 120000, 160000 ]
      backbone:
        type: SparseEncoder
        in_channels: 5
        sparse_shape: [ 1440, 1440, 41 ]
        output_channels: 128
        order:
          - conv
          - norm
          - act
        encoder_channels:
          - [ 16, 16, 32 ]
          - [ 32, 32, 64 ]
          - [ 64, 64, 128 ]
          - [ 128, 128 ]
        encoder_paddings:
          - [ 0, 0, 1 ]
          - [ 0, 0, 1 ]
          - [ 0, 0, [ 1, 1, 0 ] ]
          - [ 0, 0 ]
        block_type: basicblock
    camera:
      backbone:
        type: SwiftFormer_L1
        mlp_ratio: 4
        drop_rate: 0.
        drop_path_rate: 0.2
        num_classes: 1000
        down_patch_size: 3
        down_stride: 2
        down_pad: 1
        use_layer_scale: True
        fork_feat: True
        pretrained: True
        init_cfg:
          type: Pretrained
          checkpoint: http://rean-song-bucket.oss-cn-guangzhou.aliyuncs.com/autoDL/BEVFusion/ckpts/SwiftFormer_L1.pth
        vit_num: 1
        distillation: False
      neck: # same as teacher
        type: GeneralizedLSSFPN
        in_channels: [48, 96, 192, 384]
        out_channels: 256
        start_level: 0
        num_outs: 3
        norm_cfg:
          type: BN2d
          requires_grad: true
        act_cfg:
          type: ReLU
          inplace: true
        upsample_cfg:
          mode: bilinear
          align_corners: false
      vtransform:
        type: DepthLSSTransform
        in_channels: 256
        out_channels: 80
        image_size: ${image_size}
        feature_size: ${[image_size[0] // 8, image_size[1] // 8]}
        xbound: [-54.0, 54.0, 0.3]
        ybound: [-54.0, 54.0, 0.3]
        zbound: [-10.0, 10.0, 20.0]
        dbound: [1.0, 60.0, 0.5]
        downsample: 2
        height_expand: True
        add_depth_features: True
        is_xtransform: True

  # fuser
  fuser_student:
    type: ConvFuser
    in_channels: [80, 256]
    out_channels: 256
  # decoder
  decoder_student:
    backbone:
      type: SECOND
      in_channels: 256
      out_channels: [ 128, 256 ]
      layer_nums: [ 5, 5 ]
      layer_strides: [ 1, 2 ]
      norm_cfg:
        type: BN
        eps: 1.0e-3
        momentum: 0.01
      conv_cfg:
        type: Conv2d
        bias: false
    neck:
      type: SECONDFPN
      in_channels: [ 128, 256 ]
      out_channels: [ 256, 256 ]
      upsample_strides: [ 1, 2 ]
      norm_cfg:
        type: BN
        eps: 1.0e-3
        momentum: 0.01
      upsample_cfg:
        type: deconv
        bias: false
      use_conv_for_no_stride: true
  # heads
  heads_student:
    object:
      type: TransFusionHead
      in_channels: 512
      num_proposals: 200
      auxiliary: true
      hidden_channel: 128
      num_classes: 10
      num_decoder_layers: 1
      num_heads: 8
      nms_kernel_size: 3
      ffn_channel: 256
      dropout: 0.1
      bn_momentum: 0.1
      activation: relu
      train_cfg:
        dataset: nuScenes
        point_cloud_range: ${point_cloud_range}
        grid_size: [1440, 1440, 41]
        voxel_size: ${voxel_size}
        out_size_factor: 8
        gaussian_overlap: 0.1
        min_radius: 2
        pos_weight: -1
        code_weights: [ 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2 ]
        assigner:
          type: HungarianAssigner3D
          iou_calculator:
            type: BboxOverlaps3D
            coordinate: lidar
          cls_cost:
            type: FocalLossCost
            gamma: 2.0
            alpha: 0.25
            weight: 0.15
          reg_cost:
            type: BBoxBEVL1Cost
            weight: 0.25
          iou_cost:
            type: IoU3DCost
            weight: 0.25
      test_cfg:
        dataset: nuScenes
        grid_size: [1440, 1440, 41]
        out_size_factor: 8
        voxel_size: ${voxel_size[:2]}
        pc_range: ${point_cloud_range[:2]}
        nms_type: null
      common_heads:
        center: [ 2, 2 ]
        height: [ 1, 2 ]
        dim: [ 3, 2 ]
        rot: [ 2, 2 ]
        vel: [ 2, 2 ]
      bbox_coder:
        type: TransFusionBBoxCoder
        pc_range: ${point_cloud_range[:2]}
        post_center_range: [ -61.2, -61.2, -10.0, 61.2, 61.2, 10.0 ]
        score_threshold: 0.0
        out_size_factor: 8
        voxel_size: ${voxel_size[:2]}
        code_size: 10
#      tasks:
#        - [ "car" ]
#        - [ "truck", "construction_vehicle" ]
#        - [ "bus", "trailer" ]
#        - [ "barrier" ]
#        - [ "motorcycle", "bicycle" ]
#        - [ "pedestrian", "traffic_cone" ]
      loss_cls:
        type: FocalLoss
        use_sigmoid: true
        gamma: 2.0
        alpha: 0.25
        reduction: mean
        loss_weight: 1.0
      loss_heatmap:
        type: GaussianFocalLoss
        reduction: mean
        loss_weight: 1.0
      loss_bbox:
        type: L1Loss
        reduction: mean
        loss_weight: 0.25
      # For Response KD
      loss_cls_soft:
        type: Quality_Focal_Loss
        beta: 2.0
      loss_bbox_soft:
        type: SmoothL1Loss
        beta: 1.0
        reduction: none
        loss_weight: 1.0
      loss_soft_task_weights: [2, 2, 2, 1, 2, 1]
    
  # Checkpoint
  teacher_ckpt_path: '/root/autodl-tmp/pretrained/bevfusion-det.pth'
  student_ckpt_path: '/root/autodl-tmp/running/epoch_6.pth'

  # L2R
#  lr_kd_loss:
#      type: L1Loss
#      reduction: mean
#      loss_weight: 100.0
  
  # Fused feature loss
  fused_loss:
    type: Mask_Feat_Loss_Clip
    loss_weight: 1.0
  
  fused_feat_loss_scale_kd: 10.0

  # Fused affinity loss
  fused_affinity_loss:
    type: Affinity_Loss
    reduction: mean
    loss_weight: 1.0
    downsample_size: [32, 16, 8, 4]
    input_channels: 256
    use_adapt: True

  fused_affinity_loss_scale_kd: 0.25

  # cam feature loss
#  cam_loss:
#    type: Mask_Feat_Loss_Clip
#    loss_weight: 1.0

  cam_feat_loss_scale_kd: 10.0

  scale_bbox: true




# Training Strategy
optimizer:
  type: AdamW
  lr: 0.0001
  weight_decay: 0.01
  paramwise_cfg:
    custom_keys:
      absolute_pos_embed: 
        decay_mult: 0
      relative_position_bias_table:
        decay_mult: 0
      # encoders.camera.backbone:
      #   decay_mult: 0.1

optimizer_config:
  grad_clip:
    max_norm: 35
    norm_type: 2

lr_config:
  policy: CosineAnnealing
  warmup: linear
  warmup_iters: 500
  warmup_ratio: 0.33333333
  min_lr_ratio: 1.0e-3

max_epochs: 6

gt_paste_stop_epoch: -1


log_config:
  interval: 50
  hooks:
    -
      type: TextLoggerHook
    -
      type: TensorboardLoggerHook
    -
      type: WandbLoggerHook
      init_kwargs: {'project': 'BEVFusion-Lite', 'name': 'feat_swift_fused_da_lr_mean_c2c_scale_mask_relation_resp_kd_dynamic_class_loss_80_256'}